{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing the Data\n",
    "\n",
    "For text classification tasks, the first step is preprocessing the data to convert raw text into a format suitable for the model. This involves tokenizing the text and creating appropriate input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Stocks That Hit 52-Week Highs On Friday</td>\n",
       "      <td>2020-06-05 10:30:00-04:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Stocks That Hit 52-Week Highs On Wednesday</td>\n",
       "      <td>2020-06-03 10:45:00-04:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>71 Biggest Movers From Friday</td>\n",
       "      <td>2020-05-26 04:30:00-04:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>46 Stocks Moving In Friday's Mid-Day Session</td>\n",
       "      <td>2020-05-22 12:45:00-04:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>B of A Securities Maintains Neutral on Agilent...</td>\n",
       "      <td>2020-05-22 11:38:00-04:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0         0.0            Stocks That Hit 52-Week Highs On Friday   \n",
       "1         1.0         Stocks That Hit 52-Week Highs On Wednesday   \n",
       "2         2.0                      71 Biggest Movers From Friday   \n",
       "3         3.0       46 Stocks Moving In Friday's Mid-Day Session   \n",
       "4         4.0  B of A Securities Maintains Neutral on Agilent...   \n",
       "\n",
       "                        date stock  \n",
       "0  2020-06-05 10:30:00-04:00     A  \n",
       "1  2020-06-03 10:45:00-04:00     A  \n",
       "2  2020-05-26 04:30:00-04:00     A  \n",
       "3  2020-05-22 12:45:00-04:00     A  \n",
       "4  2020-05-22 11:38:00-04:00     A  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/analyst_ratings_processed.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1400469/1400469 [01:29<00:00, 15587.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Stocks That Hit 52-Week Highs On Friday</td>\n",
       "      <td>2020-06-05 10:30:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Stocks That Hit 52-Week Highs On Wednesday</td>\n",
       "      <td>2020-06-03 10:45:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>71 Biggest Movers From Friday</td>\n",
       "      <td>2020-05-26 04:30:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>46 Stocks Moving In Friday's Mid-Day Session</td>\n",
       "      <td>2020-05-22 12:45:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>B of A Securities Maintains Neutral on Agilent...</td>\n",
       "      <td>2020-05-22 11:38:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0         0.0            Stocks That Hit 52-Week Highs On Friday   \n",
       "1         1.0         Stocks That Hit 52-Week Highs On Wednesday   \n",
       "2         2.0                      71 Biggest Movers From Friday   \n",
       "3         3.0       46 Stocks Moving In Friday's Mid-Day Session   \n",
       "4         4.0  B of A Securities Maintains Neutral on Agilent...   \n",
       "\n",
       "                        date stock sentiment  \n",
       "0  2020-06-05 10:30:00-04:00     A   Neutral  \n",
       "1  2020-06-03 10:45:00-04:00     A   Neutral  \n",
       "2  2020-05-26 04:30:00-04:00     A   Neutral  \n",
       "3  2020-05-22 12:45:00-04:00     A   Neutral  \n",
       "4  2020-05-22 11:38:00-04:00     A   Neutral  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to determine sentiment\n",
    "def get_sentiment(title):\n",
    "    analysis = TextBlob(title)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "# Apply the function to our dataset\n",
    "tqdm.pandas()\n",
    "# df['sentiment'] = df['title'].apply(get_sentiment)\n",
    "df['sentiment'] = df['title'].progress_apply(get_sentiment)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "Neutral     930937\n",
       "Positive    339045\n",
       "Negative    130487\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count how many positive, negative and neutral sentiments we have\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/odai/5588-assignment-2-1/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing data: 100%|██████████| 1400469/1400469 [03:26<00:00, 6783.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function\n",
    "def tokenize_data(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "# Tokenize the data\n",
    "encoded_texts = encoded_texts = [tokenize_data(text) for text in tqdm(df['title'], desc=\"Tokenizing data\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fine-Tuning RoBERTa\n",
    "\n",
    "Fine-tuning involves adjusting the weights of a pre-trained model on your specific dataset. You would usually split your data into training and validation sets, and then train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d4/ggny_vpn0nv4f9ys1m85vyv80000gn/T/ipykernel_66869/591822293.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df['sentiment'].replace(label_dict)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Stocks That Hit 52-Week Highs On Friday</td>\n",
       "      <td>2020-06-05 10:30:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Stocks That Hit 52-Week Highs On Wednesday</td>\n",
       "      <td>2020-06-03 10:45:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>71 Biggest Movers From Friday</td>\n",
       "      <td>2020-05-26 04:30:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>46 Stocks Moving In Friday's Mid-Day Session</td>\n",
       "      <td>2020-05-22 12:45:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>B of A Securities Maintains Neutral on Agilent...</td>\n",
       "      <td>2020-05-22 11:38:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0         0.0            Stocks That Hit 52-Week Highs On Friday   \n",
       "1         1.0         Stocks That Hit 52-Week Highs On Wednesday   \n",
       "2         2.0                      71 Biggest Movers From Friday   \n",
       "3         3.0       46 Stocks Moving In Friday's Mid-Day Session   \n",
       "4         4.0  B of A Securities Maintains Neutral on Agilent...   \n",
       "\n",
       "                        date stock sentiment  label  \n",
       "0  2020-06-05 10:30:00-04:00     A   Neutral      1  \n",
       "1  2020-06-03 10:45:00-04:00     A   Neutral      1  \n",
       "2  2020-05-26 04:30:00-04:00     A   Neutral      1  \n",
       "3  2020-05-22 12:45:00-04:00     A   Neutral      1  \n",
       "4  2020-05-22 11:38:00-04:00     A   Neutral      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encoding (convert labels to integers)\n",
    "label_dict = {'Positive': 0, 'Neutral': 1, 'Negative': 2}\n",
    "df['label'] = df['sentiment'].replace(label_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for the model\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['title'], df['label'], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and create datasets\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "train_dataset = NewsDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = NewsDataset(val_encodings, val_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/odai/5588-assignment-2-1/.venv/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size for training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d774466a4f92488a8ce993bf014705f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/472659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1329, 'grad_norm': 6.12992525100708, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}\n",
      "{'loss': 1.1017, 'grad_norm': 6.898188591003418, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 1.0884, 'grad_norm': 3.1596133708953857, 'learning_rate': 3e-06, 'epoch': 0.0}\n",
      "{'loss': 1.0675, 'grad_norm': 3.6739771366119385, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 1.0053, 'grad_norm': 3.58829402923584, 'learning_rate': 5e-06, 'epoch': 0.0}\n",
      "{'loss': 0.9335, 'grad_norm': 6.8375420570373535, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 0.8061, 'grad_norm': 7.543118476867676, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 0.8409, 'grad_norm': 11.672341346740723, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 0.8353, 'grad_norm': 7.882238864898682, 'learning_rate': 9e-06, 'epoch': 0.0}\n",
      "{'loss': 0.7949, 'grad_norm': 6.433279037475586, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6632, 'grad_norm': 11.681200981140137, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7448, 'grad_norm': 7.973256587982178, 'learning_rate': 1.2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.8415, 'grad_norm': 9.6898832321167, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.0}\n",
      "{'loss': 0.621, 'grad_norm': 6.632391929626465, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6612, 'grad_norm': 11.988537788391113, 'learning_rate': 1.5e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                         \u001b[38;5;66;03m# the instantiated 🤗 Transformers model to be trained\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \u001b[38;5;66;03m# training arguments, defined above\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,         \u001b[38;5;66;03m# training dataset\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset             \u001b[38;5;66;03m# evaluation dataset\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/5588-assignment-2-1/.venv/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/5588-assignment-2-1/.venv/lib/python3.11/site-packages/transformers/trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_dict))\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation\n",
    "\n",
    "After training, you can evaluate the model using the validation set to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (393824984.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    ```\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
